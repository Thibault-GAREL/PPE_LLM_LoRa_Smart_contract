"""
check_environment.py
Script de v√©rification de l'environnement avant le fine-tuning
"""

import sys
import subprocess

def check_python_version():
    """V√©rifie la version de Python."""
    print("üêç V√©rification de Python...")
    version = sys.version_info
    print(f"   Version Python : {version.major}.{version.minor}.{version.micro}")
    
    if version.major < 3 or (version.major == 3 and version.minor < 8):
        print("   ‚ùå Python 3.8+ requis")
        return False
    else:
        print("   ‚úÖ Version Python OK")
        return True


def check_cuda():
    """V√©rifie la disponibilit√© de CUDA."""
    print("\nüî• V√©rification de CUDA...")
    try:
        import torch
        
        if torch.cuda.is_available():
            print(f"   ‚úÖ CUDA disponible")
            print(f"   GPU : {torch.cuda.get_device_name(0)}")
            print(f"   Version CUDA : {torch.version.cuda}")
            print(f"   Nombre de GPUs : {torch.cuda.device_count()}")
            
            # V√©rifier la m√©moire GPU
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                total_memory = props.total_memory / 1e9
                print(f"   GPU {i} - M√©moire totale : {total_memory:.2f} GB")
                
                if total_memory < 8:
                    print(f"   ‚ö†Ô∏è  GPU {i} a moins de 8GB, risque de m√©moire insuffisante")
                    print(f"      ‚Üí R√©duisez batch_size et max_length")
            
            return True
        else:
            print("   ‚ö†Ô∏è  CUDA non disponible - l'entra√Ænement sera sur CPU (tr√®s lent)")
            print("      ‚Üí Installez PyTorch avec support CUDA :")
            print("      pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
            return False
            
    except ImportError:
        print("   ‚ùå PyTorch non install√©")
        return False


def check_packages():
    """V√©rifie l'installation des packages requis."""
    print("\nüì¶ V√©rification des packages...")
    
    required_packages = {
        'torch': 'PyTorch',
        'transformers': 'Transformers',
        'datasets': 'Datasets',
        'peft': 'PEFT (LoRA)',
        'bitsandbytes': 'BitsAndBytes',
        'pandas': 'Pandas',
        'sklearn': 'Scikit-learn',
        'tqdm': 'TQDM',
    }
    
    all_ok = True
    
    for package, name in required_packages.items():
        try:
            __import__(package)
            print(f"   ‚úÖ {name}")
        except ImportError:
            print(f"   ‚ùå {name} non install√©")
            all_ok = False
    
    if not all_ok:
        print("\n   ‚Üí Installez les packages manquants avec :")
        print("   pip install -r requirements.txt")
    
    return all_ok


def check_dataset():
    """V√©rifie la pr√©sence du dataset."""
    print("\nüìä V√©rification du dataset...")
    
    import os
    import pandas as pd
    
    dataset_path = "archive/SC_Vuln_8label.csv"
    
    if not os.path.exists(dataset_path):
        print(f"   ‚ùå Dataset non trouv√© : {dataset_path}")
        print("      ‚Üí Assurez-vous que le fichier existe")
        return False
    
    print(f"   ‚úÖ Dataset trouv√© : {dataset_path}")
    
    # Charger et v√©rifier le dataset
    try:
        df = pd.read_csv(dataset_path)
        print(f"   Nombre de lignes : {len(df)}")
        
        # V√©rifier les colonnes requises
        required_cols = ['code', 'label_encoded']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            print(f"   ‚ùå Colonnes manquantes : {missing_cols}")
            return False
        
        print(f"   ‚úÖ Colonnes OK : {list(df.columns)}")
        
        # Distribution des labels
        print("\n   Distribution des labels :")
        for label, count in df['label_encoded'].value_counts().sort_index().items():
            print(f"      Label {int(label)} : {count} contrats")
        
        return True
        
    except Exception as e:
        print(f"   ‚ùå Erreur lors du chargement : {e}")
        return False


def check_disk_space():
    """V√©rifie l'espace disque disponible."""
    print("\nüíæ V√©rification de l'espace disque...")
    
    import shutil
    
    total, used, free = shutil.disk_usage(".")
    free_gb = free / (2**30)
    
    print(f"   Espace libre : {free_gb:.2f} GB")
    
    if free_gb < 20:
        print("   ‚ö†Ô∏è  Moins de 20GB disponibles")
        print("      ‚Üí Le fine-tuning n√©cessite ~20GB pour le mod√®le et les checkpoints")
        return False
    else:
        print("   ‚úÖ Espace disque suffisant")
        return True


def check_memory():
    """V√©rifie la RAM disponible."""
    print("\nüß† V√©rification de la RAM...")
    
    try:
        import psutil
        
        mem = psutil.virtual_memory()
        total_gb = mem.total / (2**30)
        available_gb = mem.available / (2**30)
        
        print(f"   RAM totale : {total_gb:.2f} GB")
        print(f"   RAM disponible : {available_gb:.2f} GB")
        
        if available_gb < 8:
            print("   ‚ö†Ô∏è  Moins de 8GB de RAM disponible")
            print("      ‚Üí Fermez les applications inutiles")
            return False
        else:
            print("   ‚úÖ RAM suffisante")
            return True
            
    except ImportError:
        print("   ‚ö†Ô∏è  psutil non install√© (pip install psutil)")
        print("   V√©rification de la RAM ignor√©e")
        return True


def estimate_training_time():
    """Estime le temps d'entra√Ænement."""
    print("\n‚è±Ô∏è  Estimation du temps d'entra√Ænement...")
    
    try:
        import torch
        
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            
            # Estimations approximatives
            estimates = {
                "RTX 4090": "1-2 heures",
                "RTX 3090": "2-4 heures",
                "RTX 3080": "3-5 heures",
                "RTX 3060": "4-8 heures",
                "RTX 2080": "5-10 heures",
                "default": "4-8 heures (selon le GPU)"
            }
            
            time_estimate = "inconnue"
            for gpu_model, estimate in estimates.items():
                if gpu_model in gpu_name:
                    time_estimate = estimate
                    break
            
            if time_estimate == "inconnue":
                time_estimate = estimates["default"]
            
            print(f"   GPU : {gpu_name}")
            print(f"   Temps estim√© (3 √©poques) : {time_estimate}")
            
        else:
            print("   CPU uniquement : plusieurs jours (non recommand√©)")
            
    except Exception as e:
        print(f"   Impossible d'estimer : {e}")


def run_test_inference():
    """Test rapide d'inf√©rence."""
    print("\nüß™ Test rapide d'inf√©rence...")
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch
        
        print("   Chargement d'un petit mod√®le de test...")
        model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        print("   ‚úÖ Chargement OK")
        
        # Test simple
        prompt = "Hello"
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=5)
        
        print("   ‚úÖ G√©n√©ration OK")
        print("   Le syst√®me est pr√™t pour le fine-tuning !")
        
        return True
        
    except Exception as e:
        print(f"   ‚ùå Erreur lors du test : {e}")
        print("   ‚Üí V√©rifiez votre installation")
        return False


def main():
    """Fonction principale."""
    print("="*60)
    print("V√âRIFICATION DE L'ENVIRONNEMENT")
    print("Fine-tuning LLaMA avec LoRA")
    print("="*60)
    
    checks = {
        "Python": check_python_version(),
        "CUDA": check_cuda(),
        "Packages": check_packages(),
        "Dataset": check_dataset(),
        "Espace disque": check_disk_space(),
        "RAM": check_memory(),
    }
    
    print("\n" + "="*60)
    print("R√âSUM√â")
    print("="*60)
    
    for check_name, result in checks.items():
        status = "‚úÖ" if result else "‚ùå"
        print(f"{status} {check_name}")
    
    estimate_training_time()
    
    all_ok = all(checks.values())
    
    if all_ok:
        print("\n" + "="*60)
        print("‚úÖ ENVIRONNEMENT PR√äT POUR LE FINE-TUNING")
        print("="*60)
        print("\nVous pouvez lancer l'entra√Ænement avec :")
        print("python fine_tune_llama_lora.py")
        
        # Test optionnel
        print("\n" + "="*60)
        response = input("\nVoulez-vous faire un test d'inf√©rence rapide ? (y/n): ")
        if response.lower() == 'y':
            run_test_inference()
        
    else:
        print("\n" + "="*60)
        print("‚ö†Ô∏è  PROBL√àMES D√âTECT√âS")
        print("="*60)
        print("\nCorrigez les probl√®mes avant de lancer l'entra√Ænement.")
        print("Consultez le README.md pour plus d'informations.")


if __name__ == "__main__":
    main()
