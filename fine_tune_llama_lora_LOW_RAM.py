"""
fine_tune_llama_lora_LOW_RAM.py
Version OPTIMIS√âE pour machines avec peu de RAM/VRAM
"""

import os
import pandas as pd
import torch
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType
)
from sklearn.model_selection import train_test_split
import json
import gc

# ========================================
# CONFIGURATION OPTIMIS√âE POUR LOW RAM
# ========================================
CONFIG = {
    # ‚ö†Ô∏è MOD√àLE PLUS L√âGER - Choisissez selon votre RAM :
    # Option 1 : TinyLlama (1GB VRAM minimum) - RECOMMAND√â
    "model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    
    # Option 2 : Phi-2 (4GB VRAM)
    # "model_name": "microsoft/phi-2",
    
    # Option 3 : LLaMA 3.2 1B (2-3GB VRAM)
    # "model_name": "meta-llama/Llama-3.2-1B",
    
    # Dataset - LIMIT√â pour tester
    "dataset_path": "archive/SC_Vuln_8label.csv",
    "test_size": 0.2,
    "random_state": 42,
    "max_samples": 500,  # ‚ö†Ô∏è LIMIT√â √† 500 pour √©conomiser RAM
    
    # LoRA parameters - R√âDUITS
    "lora_r": 8,               # ‚¨áÔ∏è R√©duit de 16 √† 8
    "lora_alpha": 16,          # ‚¨áÔ∏è R√©duit de 32 √† 16
    "lora_dropout": 0.05,
    "target_modules": ["q_proj", "v_proj"],  # ‚ö†Ô∏è Seulement 2 modules au lieu de 4
    
    # Training parameters - OPTIMIS√âS
    "output_dir": "./llama_lora_solidity_lowram",
    "num_train_epochs": 2,     # ‚¨áÔ∏è R√©duit de 3 √† 2
    "per_device_train_batch_size": 1,  # ‚ö†Ô∏è BATCH SIZE = 1
    "per_device_eval_batch_size": 1,
    "gradient_accumulation_steps": 16,  # ‚¨ÜÔ∏è Augment√© pour compenser
    "learning_rate": 2e-4,
    "max_grad_norm": 0.3,
    "warmup_ratio": 0.03,
    "lr_scheduler_type": "cosine",
    
    # Logging - ESPAC√â pour √©conomiser
    "logging_steps": 50,       # ‚¨ÜÔ∏è Augment√©
    "save_steps": 200,         # ‚¨ÜÔ∏è Augment√©
    "eval_steps": 200,         # ‚¨ÜÔ∏è Augment√©
    
    # Autres - OPTIMIS√âS
    "max_length": 1024,        # ‚¨áÔ∏è R√âDUIT de 2048 √† 1024
    "use_4bit": True,
    "use_8bit": False,         # D√©commenter si vous pr√©f√©rez 8-bit au lieu de 4-bit
}

LABELS_8 = {
    0: "Block number dependency (BN)",
    1: "Dangerous delegatecall (DE)",
    2: "Ether frozen (EF)",
    3: "Ether strict equality (SE)",
    4: "Integer overflow (OF)",
    5: "Reentrancy (RE)",
    6: "Timestamp dependency (TP)",
    7: "Unchecked external call (UC)",
    8: "Normal"
}


def free_memory():
    """Lib√®re la m√©moire GPU et RAM."""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


def create_prompt(code, label=None):
    """Cr√©e le prompt pour l'entra√Ænement."""
    labels_description = "\n".join([
        f"- {i} si le contrat a une vuln√©rabilit√© de type \"{desc}\""
        if i < 8 else f"- {i} si le contrat est normal (sans vuln√©rabilit√©)"
        for i, desc in LABELS_8.items()
    ])
    
    prompt = f"""Analyse ce contrat Solidity et identifie s'il contient une vuln√©rabilit√©.

R√©ponds UNIQUEMENT avec UN SEUL chiffre entre 0 et 8 :
{labels_description}

IMPORTANT : R√©ponds UNIQUEMENT avec le chiffre, rien d'autre.

Contrat Solidity √† analyser :
{code}

R√©ponse (un seul chiffre) :"""
    
    if label is not None:
        prompt += f" {label}"
    
    return prompt


def load_and_prepare_data(csv_path, test_size=0.2, random_state=42, max_samples=None):
    """Charge et pr√©pare le dataset (version all√©g√©e)."""
    print(f"\nüìÅ Chargement du dataset : {csv_path}")
    
    df = pd.read_csv(csv_path)
    print(f"   Nombre total de contrats : {len(df)}")
    
    # Nettoyer
    df = df.dropna(subset=['code', 'label_encoded'])
    df['label_encoded'] = df['label_encoded'].astype(int)
    
    # ‚ö†Ô∏è LIMITER le nombre d'√©chantillons
    if max_samples and len(df) > max_samples:
        print(f"\n   ‚ö†Ô∏è  LIMITATION √† {max_samples} contrats pour √©conomiser RAM")
        df = df.sample(n=max_samples, random_state=random_state)
        df = df.reset_index(drop=True)
    
    print(f"   Apr√®s nettoyage : {len(df)} contrats")
    print(f"\n   Distribution des labels :")
    for label, count in df['label_encoded'].value_counts().sort_index().items():
        print(f"      {label} ({LABELS_8[label]}): {count}")
    
    # Cr√©er les prompts
    print("\n   Cr√©ation des prompts...")
    df['text'] = df.apply(
        lambda row: create_prompt(row['code'], row['label_encoded']),
        axis=1
    )
    
    # Split train/test
    train_df, test_df = train_test_split(
        df,
        test_size=test_size,
        random_state=random_state,
        stratify=df['label_encoded']
    )
    
    print(f"\n   üìä Split des donn√©es :")
    print(f"      Train : {len(train_df)} contrats")
    print(f"      Test  : {len(test_df)} contrats")
    
    # Convertir en Dataset
    train_dataset = Dataset.from_pandas(train_df[['text']])
    test_dataset = Dataset.from_pandas(test_df[['text']])
    
    # Lib√©rer m√©moire
    del df, train_df
    free_memory()
    
    return train_dataset, test_dataset, test_df


def load_model_and_tokenizer(model_name, use_4bit=True):
    """Charge le mod√®le et tokenizer (version optimis√©e)."""
    print(f"\nü§ñ Chargement du mod√®le : {model_name}")
    
    if use_4bit:
        from transformers import BitsAndBytesConfig
        
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True,  # ‚ö†Ô∏è IMPORTANT
        )
    else:
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,  # ‚ö†Ô∏è IMPORTANT
        )
    
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        model.config.pad_token_id = model.config.eos_token_id
    
    print(f"   ‚úÖ Mod√®le charg√©")
    
    # Lib√©rer m√©moire
    free_memory()
    
    return model, tokenizer


def setup_lora(model, config):
    """Configure LoRA (version optimis√©e)."""
    print(f"\n‚öôÔ∏è  Configuration de LoRA")
    
    model = prepare_model_for_kbit_training(model)
    
    lora_config = LoraConfig(
        r=config["lora_r"],
        lora_alpha=config["lora_alpha"],
        target_modules=config["target_modules"],
        lora_dropout=config["lora_dropout"],
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )
    
    model = get_peft_model(model, lora_config)
    
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    all_params = sum(p.numel() for p in model.parameters())
    
    print(f"   Param√®tres entra√Ænables : {trainable_params:,}")
    print(f"   Tous les param√®tres : {all_params:,}")
    print(f"   Pourcentage entra√Ænable : {100 * trainable_params / all_params:.2f}%")
    
    # Lib√©rer m√©moire
    free_memory()
    
    return model


def tokenize_function(examples, tokenizer, max_length):
    """Tokenize les exemples."""
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=max_length,
        padding="max_length",
        return_tensors="pt"
    )


def train_model(model, tokenizer, train_dataset, test_dataset, config):
    """Entra√Æne le mod√®le (version optimis√©e)."""
    print(f"\nüèãÔ∏è  D√©but de l'entra√Ænement")
    
    # Tokenizer les datasets
    print("   Tokenization des donn√©es...")
    tokenized_train = train_dataset.map(
        lambda x: tokenize_function(x, tokenizer, config["max_length"]),
        batched=True,
        remove_columns=train_dataset.column_names,
        desc="Tokenizing train"
    )
    
    tokenized_test = test_dataset.map(
        lambda x: tokenize_function(x, tokenizer, config["max_length"]),
        batched=True,
        remove_columns=test_dataset.column_names,
        desc="Tokenizing test"
    )
    
    # Lib√©rer m√©moire
    free_memory()
    
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )
    
    # Arguments d'entra√Ænement OPTIMIS√âS
    training_args = TrainingArguments(
        output_dir=config["output_dir"],
        num_train_epochs=config["num_train_epochs"],
        per_device_train_batch_size=config["per_device_train_batch_size"],
        per_device_eval_batch_size=config["per_device_eval_batch_size"],
        gradient_accumulation_steps=config["gradient_accumulation_steps"],
        learning_rate=config["learning_rate"],
        max_grad_norm=config["max_grad_norm"],
        warmup_ratio=config["warmup_ratio"],
        lr_scheduler_type=config["lr_scheduler_type"],
        logging_steps=config["logging_steps"],
        save_steps=config["save_steps"],
        eval_steps=config["eval_steps"],
        evaluation_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        fp16=True,
        report_to="none",
        save_total_limit=2,  # ‚ö†Ô∏è Garde seulement 2 checkpoints
        gradient_checkpointing=True,  # ‚ö†Ô∏è IMPORTANT pour √©conomiser RAM
        optim="paged_adamw_8bit",  # ‚ö†Ô∏è Optimiseur 8-bit
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_test,
        data_collator=data_collator,
    )
    
    print("   üöÄ Lancement de l'entra√Ænement...")
    print(f"   ‚ö†Ô∏è  Cela peut prendre 1-3 heures (mod√®le l√©ger)")
    
    trainer.train()
    
    print(f"\n   ‚úÖ Entra√Ænement termin√© !")
    
    # Sauvegarder
    final_model_path = os.path.join(config["output_dir"], "final_model")
    trainer.model.save_pretrained(final_model_path)
    tokenizer.save_pretrained(final_model_path)
    
    print(f"   üíæ Mod√®le sauvegard√© dans : {final_model_path}")
    
    # Lib√©rer m√©moire
    free_memory()
    
    return trainer


def evaluate_model(model, tokenizer, test_df, max_length=1024):
    """√âvalue le mod√®le."""
    print(f"\nüìä √âvaluation du mod√®le")
    
    predictions = []
    true_labels = []
    
    model.eval()
    
    # Limiter l'√©valuation pour √©conomiser du temps
    eval_samples = min(100, len(test_df))
    print(f"   √âvaluation sur {eval_samples} √©chantillons")
    
    for idx, row in test_df.head(eval_samples).iterrows():
        prompt = create_prompt(row['code'])
        
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=max_length)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=5,
                temperature=0.1,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response[len(prompt):].strip()
        
        predicted_label = -1
        for char in response:
            if char.isdigit() and int(char) in range(9):
                predicted_label = int(char)
                break
        
        predictions.append(predicted_label)
        true_labels.append(row['label_encoded'])
        
        if (idx + 1) % 10 == 0:
            print(f"   Progression : {idx + 1}/{eval_samples}")
            free_memory()
    
    correct = sum(p == t for p, t in zip(predictions, true_labels))
    accuracy = correct / len(predictions) * 100
    
    print(f"\n   ‚úÖ Pr√©cision sur {eval_samples} √©chantillons : {accuracy:.2f}%")
    
    return predictions, true_labels, accuracy


def save_config(config, output_dir):
    """Sauvegarde la configuration."""
    config_path = os.path.join(output_dir, "training_config.json")
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    print(f"\nüíæ Configuration sauvegard√©e : {config_path}")


if __name__ == "__main__":
    print("="*60)
    print("FINE-TUNING DE LLAMA AVEC LoRA (VERSION LOW RAM)")
    print("="*60)
    
    # V√©rifier CUDA
    if torch.cuda.is_available():
        print(f"\n‚úÖ GPU disponible : {torch.cuda.get_device_name(0)}")
        mem = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"   M√©moire GPU : {mem:.2f} GB")
        
        if mem < 4:
            print("\n   ‚ö†Ô∏è  ATTENTION : Moins de 4GB de VRAM")
            print("   ‚Üí Utilisez TinyLlama (d√©j√† configur√©)")
            print("   ‚Üí R√©duisez max_samples √† 200 si probl√®me")
    else:
        print("\n‚ö†Ô∏è  Pas de GPU d√©tect√© - entra√Ænement tr√®s lent")
    
    print("\n" + "="*60)
    print("CONFIGURATION LOW RAM ACTIVE")
    print("="*60)
    print(f"Mod√®le : {CONFIG['model_name']}")
    print(f"Max samples : {CONFIG['max_samples']}")
    print(f"Batch size : {CONFIG['per_device_train_batch_size']}")
    print(f"Max length : {CONFIG['max_length']}")
    print(f"√âpoques : {CONFIG['num_train_epochs']}")
    print("="*60)
    
    response = input("\nContinuer ? (y/n): ")
    if response.lower() != 'y':
        print("Annul√©")
        exit(0)
    
    # 1. Charger les donn√©es
    train_dataset, test_dataset, test_df = load_and_prepare_data(
        CONFIG["dataset_path"],
        CONFIG["test_size"],
        CONFIG["random_state"],
        CONFIG["max_samples"]
    )
    
    # 2. Charger le mod√®le
    model, tokenizer = load_model_and_tokenizer(
        CONFIG["model_name"],
        CONFIG["use_4bit"]
    )
    
    # 3. Configurer LoRA
    model = setup_lora(model, CONFIG)
    
    # 4. Sauvegarder config
    os.makedirs(CONFIG["output_dir"], exist_ok=True)
    save_config(CONFIG, CONFIG["output_dir"])
    
    # 5. Entra√Æner
    trainer = train_model(model, tokenizer, train_dataset, test_dataset, CONFIG)
    
    # 6. √âvaluer
    predictions, true_labels, accuracy = evaluate_model(
        model, tokenizer, test_df, CONFIG["max_length"]
    )
    
    print("\n" + "="*60)
    print("ENTRA√éNEMENT TERMIN√â !")
    print("="*60)
    print(f"\n‚úÖ Mod√®le sauvegard√© dans : {CONFIG['output_dir']}/final_model")
    print(f"‚úÖ Pr√©cision : {accuracy:.2f}%")
