"""
convert_to_ollama.py
Script pour convertir le mod√®le fine-tun√© en format GGUF pour Ollama
"""

import os
import shutil
import subprocess
import json

# Configuration
MODEL_LORA_PATH = "./llama_lora_solidity_finetuned/final_model"
MODEL_BASE = "meta-llama/Llama-3.2-3B"
OUTPUT_DIR = "./ollama_model"
MODELFILE_PATH = os.path.join(OUTPUT_DIR, "Modelfile")
GGUF_PATH = os.path.join(OUTPUT_DIR, "model.gguf")

LABELS_8 = {
    0: "Block number dependency (BN)",
    1: "Dangerous delegatecall (DE)",
    2: "Ether frozen (EF)",
    3: "Ether strict equality (SE)",
    4: "Integer overflow (OF)",
    5: "Reentrancy (RE)",
    6: "Timestamp dependency (TP)",
    7: "Unchecked external call (UC)",
    8: "Normal"
}


def merge_lora_weights():
    """
    Fusionne les poids LoRA avec le mod√®le de base.
    """
    print("üîÄ Fusion des poids LoRA avec le mod√®le de base...")
    
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import PeftModel
    import torch
    
    # Charger le mod√®le de base
    print(f"   Chargement du mod√®le de base : {MODEL_BASE}")
    base_model = AutoModelForCausalLM.from_pretrained(
        MODEL_BASE,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # Charger et fusionner LoRA
    print(f"   Chargement des poids LoRA : {MODEL_LORA_PATH}")
    model = PeftModel.from_pretrained(base_model, MODEL_LORA_PATH)
    
    print("   Fusion en cours...")
    model = model.merge_and_unload()
    
    # Sauvegarder le mod√®le fusionn√©
    merged_path = os.path.join(OUTPUT_DIR, "merged_model")
    os.makedirs(merged_path, exist_ok=True)
    
    print(f"   Sauvegarde du mod√®le fusionn√© : {merged_path}")
    model.save_pretrained(merged_path)
    
    # Sauvegarder le tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_LORA_PATH)
    tokenizer.save_pretrained(merged_path)
    
    print("   ‚úÖ Fusion termin√©e")
    return merged_path


def convert_to_gguf(merged_model_path):
    """
    Convertit le mod√®le fusionn√© en format GGUF.
    
    Note: N√©cessite llama.cpp
    """
    print("\nüì¶ Conversion en format GGUF...")
    
    # V√©rifier si llama.cpp est disponible
    llama_cpp_path = input(
        "Entrez le chemin vers llama.cpp (ou appuyez sur Entr√©e pour ignorer) : "
    ).strip()
    
    if not llama_cpp_path:
        print("   ‚ö†Ô∏è  Conversion GGUF ignor√©e")
        print("   Pour convertir manuellement :")
        print(f"   1. Clonez llama.cpp : git clone https://github.com/ggerganov/llama.cpp")
        print(f"   2. Compilez : cd llama.cpp && make")
        print(f"   3. Convertissez : python convert.py {merged_model_path}")
        print(f"   4. Quantifiez : ./quantize {merged_model_path}/ggml-model-f16.gguf model.gguf q4_0")
        return None
    
    convert_script = os.path.join(llama_cpp_path, "convert.py")
    
    if not os.path.exists(convert_script):
        print(f"   ‚ùå Script de conversion non trouv√© : {convert_script}")
        return None
    
    # Convertir en GGUF
    print("   Conversion en cours...")
    try:
        subprocess.run([
            "python",
            convert_script,
            merged_model_path,
            "--outtype", "f16",
            "--outfile", GGUF_PATH
        ], check=True)
        
        print(f"   ‚úÖ GGUF cr√©√© : {GGUF_PATH}")
        return GGUF_PATH
        
    except subprocess.CalledProcessError as e:
        print(f"   ‚ùå Erreur lors de la conversion : {e}")
        return None


def create_modelfile():
    """
    Cr√©e le Modelfile pour Ollama.
    """
    print("\nüìù Cr√©ation du Modelfile pour Ollama...")
    
    # Template du prompt syst√®me
    labels_description = "\n".join([
        f"- {i} si le contrat a une vuln√©rabilit√© de type \"{desc}\""
        if i < 8 else f"- {i} si le contrat est normal (sans vuln√©rabilit√©)"
        for i, desc in LABELS_8.items()
    ])
    
    system_prompt = f"""Tu es un expert en s√©curit√© des smart contracts Solidity. Ta t√¢che est d'analyser du code Solidity et d'identifier les vuln√©rabilit√©s.

Pour chaque contrat, r√©ponds UNIQUEMENT avec UN SEUL chiffre entre 0 et 8 :
{labels_description}

IMPORTANT : R√©ponds UNIQUEMENT avec le chiffre correspondant √† la vuln√©rabilit√© d√©tect√©e, rien d'autre."""
    
    # Cr√©er le Modelfile
    modelfile_content = f"""FROM {GGUF_PATH if os.path.exists(GGUF_PATH) else './model.gguf'}

# Param√®tres du mod√®le
PARAMETER temperature 0.1
PARAMETER top_p 0.9
PARAMETER stop \"<|endoftext|>\"
PARAMETER stop \"</s>\"

# Template du syst√®me
TEMPLATE \"\"\"{{{{ if .System }}}}{{{{ .System }}}}{{{{ end }}}}

Contrat Solidity √† analyser :
{{{{ .Prompt }}}}

R√©ponse (un seul chiffre) :\"\"\"

# Prompt syst√®me
SYSTEM \"\"\"
{system_prompt}
\"\"\"
"""
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    with open(MODELFILE_PATH, 'w') as f:
        f.write(modelfile_content)
    
    print(f"   ‚úÖ Modelfile cr√©√© : {MODELFILE_PATH}")
    return MODELFILE_PATH


def create_ollama_model():
    """
    Cr√©e le mod√®le dans Ollama.
    """
    print("\nüöÄ Cr√©ation du mod√®le dans Ollama...")
    
    model_name = input("Entrez le nom du mod√®le Ollama (ex: llama-solidity) : ").strip()
    
    if not model_name:
        model_name = "llama-solidity"
    
    try:
        # Cr√©er le mod√®le
        subprocess.run([
            "ollama",
            "create",
            model_name,
            "-f",
            MODELFILE_PATH
        ], check=True)
        
        print(f"\n   ‚úÖ Mod√®le cr√©√© : {model_name}")
        print(f"\n   Vous pouvez maintenant l'utiliser avec :")
        print(f"   ollama run {model_name}")
        
        return model_name
        
    except subprocess.CalledProcessError as e:
        print(f"   ‚ùå Erreur lors de la cr√©ation : {e}")
        print("\n   Cr√©ez le mod√®le manuellement avec :")
        print(f"   ollama create {model_name} -f {MODELFILE_PATH}")
        return None
    except FileNotFoundError:
        print("   ‚ùå Ollama n'est pas install√© ou n'est pas dans le PATH")
        print("\n   Installez Ollama depuis : https://ollama.ai")
        print(f"\n   Puis cr√©ez le mod√®le avec :")
        print(f"   ollama create {model_name} -f {MODELFILE_PATH}")
        return None


def test_ollama_model(model_name):
    """
    Teste le mod√®le Ollama cr√©√©.
    """
    if not model_name:
        return
    
    print(f"\nüß™ Test du mod√®le {model_name}...")
    
    test_contract = """pragma solidity ^0.4.0;
contract Vulnerable {
    mapping(address => uint) balances;
    
    function withdraw() public {
        uint amount = balances[msg.sender];
        msg.sender.call.value(amount)();
        balances[msg.sender] = 0;
    }
}"""
    
    print("\n   Code de test (Reentrancy) :")
    print("   " + test_contract.replace("\n", "\n   "))
    
    try:
        result = subprocess.run([
            "ollama",
            "run",
            model_name,
            test_contract
        ], capture_output=True, text=True, timeout=30)
        
        print(f"\n   R√©ponse du mod√®le : {result.stdout.strip()}")
        print("   Attendu : 5 (Reentrancy)")
        
    except subprocess.TimeoutExpired:
        print("   ‚ö†Ô∏è  Timeout - le mod√®le met trop de temps √† r√©pondre")
    except Exception as e:
        print(f"   ‚ùå Erreur lors du test : {e}")


def create_usage_script():
    """
    Cr√©e un script Python pour utiliser le mod√®le Ollama.
    """
    print("\nüìÑ Cr√©ation du script d'utilisation...")
    
    script_content = '''"""
use_ollama_model.py
Script pour utiliser le mod√®le Ollama fine-tun√©
"""

import requests
import json

MODEL_NAME = "llama-solidity"  # √Ä modifier si vous avez choisi un autre nom
OLLAMA_URL = "http://localhost:11434/api/generate"

LABELS_8 = {
    0: "Block number dependency (BN)",
    1: "Dangerous delegatecall (DE)",
    2: "Ether frozen (EF)",
    3: "Ether strict equality (SE)",
    4: "Integer overflow (OF)",
    5: "Reentrancy (RE)",
    6: "Timestamp dependency (TP)",
    7: "Unchecked external call (UC)",
    8: "Normal"
}


def classify_contract(code):
    """
    Classifie un contrat Solidity avec le mod√®le Ollama.
    """
    response = requests.post(
        OLLAMA_URL,
        json={
            'model': MODEL_NAME,
            'prompt': code,
            'stream': False
        },
        timeout=60
    )
    
    if response.status_code == 200:
        result = response.json().get('response', '').strip()
        
        # Extraire le chiffre
        for char in result:
            if char.isdigit() and int(char) in range(9):
                return int(char)
    
    return -1


if __name__ == "__main__":
    # Exemple d'utilisation
    test_code = """pragma solidity ^0.4.0;
contract Test {
    function withdraw() public {
        msg.sender.call.value(balance)();
        balance = 0;
    }
}"""
    
    print("Classification d'un contrat de test...")
    prediction = classify_contract(test_code)
    
    if prediction != -1:
        print(f"R√©sultat : {prediction} - {LABELS_8[prediction]}")
    else:
        print("Erreur de classification")
'''
    
    script_path = os.path.join(OUTPUT_DIR, "use_ollama_model.py")
    
    with open(script_path, 'w') as f:
        f.write(script_content)
    
    print(f"   ‚úÖ Script cr√©√© : {script_path}")


def main():
    """
    Fonction principale.
    """
    print("="*60)
    print("CONVERSION DU MOD√àLE POUR OLLAMA")
    print("="*60)
    
    # V√©rifier que le mod√®le LoRA existe
    if not os.path.exists(MODEL_LORA_PATH):
        print(f"\n‚ùå Mod√®le LoRA non trouv√© : {MODEL_LORA_PATH}")
        print("   Entra√Ænez d'abord le mod√®le avec fine_tune_llama_lora.py")
        return
    
    print(f"\n‚úÖ Mod√®le LoRA trouv√© : {MODEL_LORA_PATH}")
    
    # √âtapes de conversion
    print("\n" + "="*60)
    print("√âTAPES DE CONVERSION")
    print("="*60)
    print("1. Fusion des poids LoRA avec le mod√®le de base")
    print("2. Conversion en format GGUF (optionnel)")
    print("3. Cr√©ation du Modelfile pour Ollama")
    print("4. Cr√©ation du mod√®le dans Ollama")
    print("5. Test du mod√®le")
    
    response = input("\nContinuer ? (y/n): ")
    if response.lower() != 'y':
        print("Conversion annul√©e")
        return
    
    # 1. Fusionner les poids
    merged_path = merge_lora_weights()
    
    # 2. Convertir en GGUF (optionnel)
    convert_to_gguf(merged_path)
    
    # 3. Cr√©er le Modelfile
    create_modelfile()
    
    # 4. Cr√©er le mod√®le dans Ollama
    model_name = create_ollama_model()
    
    # 5. Tester le mod√®le
    if model_name:
        test_ollama_model(model_name)
    
    # Cr√©er le script d'utilisation
    create_usage_script()
    
    print("\n" + "="*60)
    print("CONVERSION TERMIN√âE")
    print("="*60)
    
    if model_name:
        print(f"\n‚úÖ Mod√®le Ollama cr√©√© : {model_name}")
        print(f"\nPour l'utiliser :")
        print(f"  ollama run {model_name}")
        print(f"\nOu avec le script Python :")
        print(f"  python {OUTPUT_DIR}/use_ollama_model.py")
    else:
        print(f"\n‚ö†Ô∏è  Mod√®le non cr√©√© dans Ollama")
        print(f"   Fichiers disponibles dans : {OUTPUT_DIR}")
        print(f"   Consultez le README pour la proc√©dure manuelle")


if __name__ == "__main__":
    main()
